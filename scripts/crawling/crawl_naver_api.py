#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ë‰´ìŠ¤ API í¬ë¡¤ëŸ¬ (ë…¸ì…˜ í‚¤ì›Œë“œ ê´€ë¦¬, ì˜¤ëŠ˜ ê¸°ì‚¬ë§Œ)

í†µí•© í¬ë¡¤ë§ + ì „ì²˜ë¦¬:
1. ë…¸ì…˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í‚¤ì›Œë“œ ë¡œë“œ (ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©)
2. Naver News API í¬ë¡¤ë§ (ì˜¤ëŠ˜ ë‚ ì§œ í•„í„°ë§)
3. aide-preprocessingì„ í†µí•œ ì „ì²˜ë¦¬ ë° DB ì €ìž¥
"""
import sys
import os
import requests
from pathlib import Path
from datetime import datetime, date

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "aide-preprocessing"))
sys.path.insert(0, str(project_root / "aide-data-core"))
sys.path.insert(0, str(project_root / "scripts"))

from dotenv import load_dotenv
load_dotenv(project_root / "aide-crawlers" / ".env")

from aide_preprocessing import PreprocessingPipeline
from aide_data_core.models import get_engine, get_session, NaverNews
from utils.notion_keywords import get_crawler_keywords


def search_naver_news(keyword: str, display: int = 100):
    """Naver News API ê²€ìƒ‰ (ìˆœìˆ˜ í¬ë¡¤ë§)

    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        display: ê²°ê³¼ ê°œìˆ˜ (ìµœëŒ€ 100)

    Returns:
        API ì‘ë‹µ JSON
    """
    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")

    if not client_id or not client_secret:
        raise ValueError("NAVER API credentials not found in .env")

    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }
    params = {
        "query": keyword,
        "display": display,
        "sort": "date"  # ìµœì‹ ìˆœ
    }

    response = requests.get(url, headers=headers, params=params, timeout=10)
    response.raise_for_status()

    return response.json()


def is_today_article(pub_date_str: str) -> bool:
    """ê¸°ì‚¬ê°€ ì˜¤ëŠ˜ ìž‘ì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
    try:
        # "Wed, 16 Oct 2024 18:30:00 +0900" í˜•ì‹
        pub_date = datetime.strptime(pub_date_str, "%a, %d %b %Y %H:%M:%S %z")
        today = date.today()
        return pub_date.date() == today
    except:
        return False


def main():
    """ë©”ì¸ í¬ë¡¤ë§ + ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    print()
    print("=" * 80)
    print("ë„¤ì´ë²„ ë‰´ìŠ¤ API í¬ë¡¤ëŸ¬ (ë…¸ì…˜ í‚¤ì›Œë“œ ê´€ë¦¬, ì˜¤ëŠ˜ ê¸°ì‚¬ë§Œ)")
    print("=" * 80)
    print(f"í¬ë¡¤ë§ ë‚ ì§œ: {date.today().isoformat()}")
    print()

    # ë…¸ì…˜ì—ì„œ í‚¤ì›Œë“œ ë¡œë“œ (ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©)
    print("ðŸ“‹ í‚¤ì›Œë“œ ë¡œë“œ ì¤‘...")
    keywords = get_crawler_keywords(fallback_to_default=True)

    if not keywords:
        print("âŒ í‚¤ì›Œë“œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return 0

    print(f"âœ… í‚¤ì›Œë“œ: {len(keywords)}ê°œ")
    print(f"   ì˜ˆìƒ í¬ë¡¤ë§: ~{len(keywords) * 100}ê°œ (í•„í„°ë§ ì „)")
    print()

    # Initialize preprocessing pipeline
    db_path = str(project_root / "aide-data-core" / "aide_dev.db").replace("\\", "/")
    db_url = os.getenv("DATABASE_URL", f"sqlite:///{db_path}")
    engine = get_engine(db_url)
    session = get_session(engine)
    pipeline = PreprocessingPipeline(session)

    total_crawled = 0
    total_today = 0
    total_saved = 0
    total_duplicates = 0

    try:
        for idx, keyword in enumerate(keywords, 1):
            print(f"[{idx}/{len(keywords)}] {keyword}")

            # Step 1: Crawling (ìˆœìˆ˜ í¬ë¡¤ë§)
            result = search_naver_news(keyword, display=100)
            items = result.get('items', [])
            total_crawled += len(items)

            # Filter today's articles only
            today_items = [item for item in items if is_today_article(item.get('pubDate', ''))]
            total_today += len(today_items)

            if not today_items:
                print(f"  í¬ë¡¤ë§: {len(items)}ê°œ â†’ ì˜¤ëŠ˜: 0ê°œ (ê±´ë„ˆë›°ê¸°)")
                continue

            print(f"  í¬ë¡¤ë§: {len(items)}ê°œ â†’ ì˜¤ëŠ˜: {len(today_items)}ê°œ", end=" ")

            # Step 2: Preprocessing (ì „ì²˜ë¦¬ + ì¤‘ë³µ í™•ì¸ + DB ì €ìž¥)
            _, saved, duplicates = pipeline.process_and_save(
                today_items,
                keyword=keyword,
                model_class=NaverNews
            )

            total_saved += saved
            total_duplicates += duplicates

            print(f"â†’ ì €ìž¥: {saved}ê°œ")

        # Final statistics
        print()
        print("=" * 80)
        print("í¬ë¡¤ë§ ìš”ì•½")
        print("=" * 80)
        print(f"ì´ í¬ë¡¤ë§: {total_crawled}ê°œ (í•„í„°ë§ ì „)")
        print(f"ì˜¤ëŠ˜ ê¸°ì‚¬: {total_today}ê°œ ({total_today/total_crawled*100:.1f}%)" if total_crawled > 0 else "ì˜¤ëŠ˜ ê¸°ì‚¬: 0ê°œ")
        print(f"ì €ìž¥: {total_saved}ê°œ")
        print(f"ì¤‘ë³µ: {total_duplicates}ê°œ")
        print()
        print("=" * 80)
        print("[SUCCESS] API í¬ë¡¤ë§ ì™„ë£Œ")
        print("=" * 80)

        return total_saved

    except Exception as e:
        print(f"\n[ERROR] {e}")
        import traceback
        traceback.print_exc()
        return 0

    finally:
        pipeline.close()


if __name__ == "__main__":
    saved = main()
    sys.exit(0 if saved >= 0 else 1)
